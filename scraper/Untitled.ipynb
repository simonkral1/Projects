{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b94bb6df-945d-449a-bde8-a176cc0690b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d1a241-a4e9-4715-9366-ec9c9639ddd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\simon\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a44fec51-e93f-466b-8e23-3f7e7a249dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'output.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 54\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[1;32m---> 54\u001b[0m     soup, posts_count \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m posts_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 41\u001b[0m, in \u001b[0;36mscrape_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     38\u001b[0m comments \u001b[38;5;241m=\u001b[39m scrape_comments(post_soup)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Save the post and its comments to CSV\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     42\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m, quoting\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mQUOTE_MINIMAL)\n\u001b[0;32m     43\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPost\u001b[39m\u001b[38;5;124m'\u001b[39m, title, author, likes, post_content])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'output.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Function to scrape comments from a post\n",
    "def scrape_comments(post_soup):\n",
    "    comments = post_soup.find_all('div', class_='comment')\n",
    "    comment_list = []\n",
    "    for comment in comments:\n",
    "        author = comment.find('a', class_='author')\n",
    "        author_name = author.text if author else \"Unknown\"\n",
    "        comment_body = comment.find('div', class_='md').get_text(strip=True) if comment.find('div', class_='md') else \"No content\"\n",
    "        comment_list.append({'author': author_name, 'comment': comment_body})\n",
    "    return comment_list\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_page(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    posts = soup.find_all('div', attrs={'class': 'thing', 'data-domain': 'self.uktravel'})\n",
    "\n",
    "    for post in posts:\n",
    "        title = post.find('p', class_=\"title\").text\n",
    "        author = post.find('a', class_='author').text\n",
    "        comments_link = post.find('a', class_='comments')['href']\n",
    "        likes = post.find(\"div\", attrs={\"class\": \"score unvoted\"}).text\n",
    "        if likes == \"â€¢\":\n",
    "            likes = \"None\"\n",
    "\n",
    "        # Fetching the post page\n",
    "        post_page = requests.get(comments_link, headers=headers)\n",
    "        post_soup = BeautifulSoup(post_page.text, 'html.parser')\n",
    "        post_content = post_soup.find('div', class_='expando').get_text(strip=True) if post_soup.find('div', class_='expando') else \"No content\"\n",
    "        \n",
    "        # Fetching comments\n",
    "        comments = scrape_comments(post_soup)\n",
    "\n",
    "        # Save the post and its comments to CSV\n",
    "        with open('output.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writerow(['Post', title, author, likes, post_content])\n",
    "            for comment in comments:\n",
    "                writer.writerow(['Comment', comment['author'], '', '', comment['comment']])\n",
    "\n",
    "    return soup, len(posts)\n",
    "\n",
    "# Main scraping function\n",
    "def main():\n",
    "    base_url = \"https://old.reddit.com/r/uktravel\"\n",
    "    counter = 0\n",
    "    while counter < 100:\n",
    "        soup, posts_count = scrape_page(base_url)\n",
    "        if posts_count == 0:\n",
    "            break\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if not next_button:\n",
    "            break\n",
    "        next_page_link = next_button.find(\"a\").attrs['href']\n",
    "        base_url = next_page_link\n",
    "        counter += posts_count\n",
    "        time.sleep(2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
