{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94bb6df-945d-449a-bde8-a176cc0690b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d1a241-a4e9-4715-9366-ec9c9639ddd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\simon\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\simon\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a44fec51-e93f-466b-8e23-3f7e7a249dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to scrape data from a single page\n",
    "def scrape_page(url):\n",
    "    # Headers to mimic a browser visit\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "    # Get the page content\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # Find all the posts on the page\n",
    "    posts = soup.find_all('div', attrs={'class': 'thing', 'data-domain': 'self.philosophy'})\n",
    "\n",
    "    # Process each post\n",
    "    for post in posts:\n",
    "        title = post.find('p', class_=\"title\").text\n",
    "        author = post.find('a', class_='author').text\n",
    "        comments = post.find('a', class_='comments').text.split()[0]\n",
    "        if comments == \"comment\":\n",
    "            comments = 0\n",
    "        likes = post.find(\"div\", attrs={\"class\": \"score likes\"}).text\n",
    "        if likes == \"â€¢\":\n",
    "            likes = \"None\"\n",
    "\n",
    "        # Save the data to CSV\n",
    "        post_data = [title, author, likes, comments]\n",
    "        with open('output.csv', 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(post_data)\n",
    "    \n",
    "    return soup, len(posts)\n",
    "\n",
    "# Main scraping function\n",
    "def main():\n",
    "    base_url = \"https://old.reddit.com/r/philosophy\"\n",
    "    counter = 0\n",
    "    while counter < 100:\n",
    "        soup, posts_count = scrape_page(base_url)\n",
    "        if posts_count == 0:\n",
    "            break  # Break if no posts were found\n",
    "        next_button = soup.find(\"span\", class_=\"next-button\")\n",
    "        if not next_button:  # Break if no next button is found\n",
    "            break\n",
    "        next_page_link = next_button.find(\"a\").attrs['href']\n",
    "        base_url = next_page_link\n",
    "        counter += posts_count\n",
    "        time.sleep(2)  # Sleep to respect Reddit's rate limits\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
